# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EhUjgz4P4QVRIkZH2Z0RPGS1DmMli7iO
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import AdaBoostClassifier
from xgboost import XGBClassifier

df = pd.read_csv("C:\\Kuliah\\kuliah-sem-6\\ml\\tubes\\titanic_data.csv")

# menampilkan 5 baris pertama
# print(df.head())

print("banyak data:",len(df.index))
df.describe()

# melihat data yang hilang
missing_data = df.isnull().sum()
missing_percent = (df.isnull().sum() / len(df)) * 100

print(missing_data)
print(missing_percent)

# Karena kolom kabin memiliki 77% data yang hilang, maka kolom kabin akan tidak digunakan
df = df.drop(columns=["Cabin"])
# kolom id hanya merupakan row identifier yang unik dan tidak penting, maka akan di drop
df = df.drop(columns=["PassengerId"])
# Ticket juga tidak penting karena hanya merupakan id ticket, maka akan di drop
df = df.drop(columns=["Ticket"])

df.head()

# isi kolom age yang hilang dengan median
df["Age"] = df["Age"].fillna(df["Age"].median())

# isi kolom fare yang hilang dengan median
df["Fare"] = df["Fare"].fillna(df["Fare"].median())

df.head()

# nama juga tidak penting, namun nama mengandung title seperti mr, miss, mrs, dll
# maka akan dilakukan ekstraksi title dari nama tersebut
df.loc[:, "Title"] = df["Name"].str.extract(r",\s*([^\.]*)\s*\.", expand=False)
df = df.drop(columns=["Name"])

# tampilkan title yang telah di extract bersamaan dengan jumlahnya
print(df["Title"].value_counts())

# Miss dan Ms memiliki arti yang sama, maka akan di gabungkan jadi 1
df["Title"] = df["Title"].replace({
    "Miss": "Ms",
    "Ms": "Ms"
})

# terdapat beberapa title yang langka seperti Rev, Dr, Col, Major, dll
# keempatnya akan di group menjadi satu yaitu title rare untuk mencegah overfit
df["Title"] = df["Title"].replace({
    "Rev": "Rare",
    "Dr": "Rare",
    "Col": "Rare",
    "Major": "Rare",
    "Don": "Rare",
    "Lady": "Rare",
    "Sir": "Rare",
    "Mlle": "Rare",
    "Jonkheer": "Rare"
})

df.head()

print(len(df.index))# SibSp adalah berapa sibling yang ada di kapal
# Parch adalah berapa parent/children yang ada di kapal
# keduanya akan digabungkan menjadi kolom FamilySize
df["FamilySize"] = df["SibSp"] + df["Parch"] + 1
df = df.drop(columns=["SibSp"])
df = df.drop(columns=["Parch"])

df.head()

# visualisasi data

sns.set(style="whitegrid")

categorical_cols = ["Pclass", "Sex", "Embarked", "Title"]

for col in categorical_cols:
    plt.figure(figsize=(6, 4))
    sns.countplot(data=df, x=col, hue="Survived", order=df[col].value_counts().index)
    plt.title(f"{col} vs Survived")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

numerical_cols = ["Age", "Fare", "FamilySize"]

for col in numerical_cols:
    plt.figure(figsize=(14, 5))

    # Histogram
    plt.subplot(1, 2, 1)
    sns.histplot(data=df, x=col, kde=True, hue="Survived", element="step")
    plt.title(f"{col} Distribution by Survived")

    # Boxplot
    plt.subplot(1, 2, 2)
    sns.boxplot(data=df, x="Survived", y=col, palette="Set2")
    plt.title(f"{col} Boxplot by Survived")

    plt.tight_layout()
    plt.show()

# melakukan one hot encoding pada kolom sex, embarked, dan title
df = pd.get_dummies(df, columns=["Sex", "Embarked", "Title"])
df.head()

# Split features and target
X = df.drop("Survived", axis=1)
y = df["Survived"]

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# membangun model decision tree

# hyperparameter tuning dengan grid search
param_grid = {
    'max_depth': [None, 3, 5, 7, 10],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 5],
    'criterion': ['gini', 'entropy']
}

# inisialisasi model
dt = DecisionTreeClassifier(random_state=42)

# Grid Search dengan 5-fold cross validation
grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

# mendapatkan best model
best_dt = grid_search.best_estimator_

# coba predik di test set
y_pred = best_dt.predict(X_test)
print("Best Parameters:", grid_search.best_params_)
print("Test Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

# Membangun model RFR

# hyperparameter tuning dengan grid search
param_grid = {
    'n_estimators': [50, 100, 150],
    'max_depth': [None, 5, 10],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# inisialisasi
rf = RandomForestClassifier(random_state=42)

# Grid Search dengan 5-fold cross validation
grid_search = GridSearchCV(
    estimator=rf,
    param_grid=param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)
grid_search.fit(X_train, y_train)

# mendapatkan best model
best_rf = grid_search.best_estimator_

# coba predik di test set
y_pred = best_rf.predict(X_test)

# Evaluation
print("Best Parameters:", grid_search.best_params_)
print("Test Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Membangun model GradientBoosting

# hyperparameter tuning dengan grid search
param_grid = {
    'n_estimators': [50, 100, 150],
    'learning_rate': [0.1, 0.2, 0.3],
    'max_depth': [None, 5, 10],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Inisialisasi model
gb = GradientBoostingClassifier(random_state=42)

# # Grid Search dengan 5-fold cross validation
grid_search = GridSearchCV(
    estimator=gb,
    param_grid=param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)
grid_search.fit(X_train, y_train)

# Dapatkan best model
best_gb = grid_search.best_estimator_

# Prediksi di test set
y_pred = best_gb.predict(X_test)

# Evaluasi
print("Best Parameters:", grid_search.best_params_)
print("Test Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Membangun model AdaBoost

# hyperparameter tuning dengan grid search
param_grid = {
    'n_estimators': [50, 100, 150],
    'learning_rate': [0.05, 0.1, 0.2, 1.0]
}

# Inisialisasi model
ab = AdaBoostClassifier(random_state=42)

# Grid Search dengan 5-fold cross validation
grid_search = GridSearchCV(
    estimator=ab,
    param_grid=param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)
grid_search.fit(X_train, y_train)

# Dapatkan best model
best_ab = grid_search.best_estimator_

# Prediksi di test set
y_pred = best_ab.predict(X_test)

# Evaluasi
print("Best Parameters:", grid_search.best_params_)
print("Test Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# membangun model XGB

# hyperparameter tuning dengan grid search
param_grid = {
    'n_estimators': [50, 100, 150],
    'max_depth': [None, 5, 10],
    'learning_rate': [0.1, 0.2, 0.3],
    'colsample_bytree': [0.5, 0.8, 1.0]
}

# Inisialisasi model
xgb = XGBClassifier(
    objective='binary:logistic',
    eval_metric='logloss',
    random_state=42
)

# Grid Search dengan 5-fold cross validation
grid_search = GridSearchCV(
    estimator=xgb,
    param_grid=param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)
grid_search.fit(X_train, y_train)

# Best model
best_xgb = grid_search.best_estimator_

# Prediksi
y_pred = best_xgb.predict(X_test)

# Evaluasi
print("Best Parameters:", grid_search.best_params_)
print("Test Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
